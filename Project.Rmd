---
title: "Final Project report -- Job Descriptions"
author: "Ruxin Liu"
date: "12/13/2020"
output: pdf_document
---

# Introduction


  As we are now in a big data world, more and more data related jobs have been created
  recently among different fields. When we are searching these job positions online,
  it is very common to see diverse descriptions, titles and requirements among different
  companies and job boards. The main goal of this project is to explore some of 
  these varieties through the use of the application programming interface (API).


# Data Procesing 

  The data sources of this project are from Adzuna and The Muse, which are both online
  career platforms. After registering for the API ID and key, we could write queries
  to obtain the data and process the data in R and all the detailed codes for data
  cleaning and processing are in the file Data Processing.Rmd. 
  
  Adzuna is an employment website where its headquarters located in the United 
  Kingdom (Wikipedia 2020). From this site, we obtain 50-page of job information
  in Britain that has keywords of data, statisticians, statistician, analyst or 
  analysts. Also, we obtain 50-page of job information in America with the same
  list of key words. Only the job information from Britain has salary listed, which
  is reasonable, since the majority of the job boards do not show the salary on 
  their pages.
  
  The Muse is another employment website founded in 2011, where its headquarters
  located in the United States (Wikipedia 2020). From this site, we obtain 60-page
  of job information from the data science category, which is very close to the 
  keywords being searched from Adzuna. 
  
```{r, echo = FALSE, message=FALSE}
library(wordcloud)
library(RColorBrewer)
library(wordcloud2)
library(tm)
library(tidyverse)
library(dplyr)
```

```{r, echo = FALSE}
# Read in the pre-processed data
muse_data <- read.csv("muse_data.csv")
Adzuna_data <- read.csv("Adzuna_data.csv")
Adzuna_us <- read.csv("Adzuna_us.csv")
```



# EDA (Exploratory Data Analysis)

### Salary Level & Job Title


  Since the UK job information on Adzuna website has predicted maximum salary in
  pounds, we could explore whether the salary level has any relationship with the
  job title. In order to do this, an indicator variable is created to determine 
  whether or not the position name includes "Statistician". From Fig.1 below, we 
  can see that overall companies predict a relatively higher salary for job titles 
  containing "Statistician" (indicator = 1). However, it is also very interesting
  to see that there are much more outliers with very large values for the job titles 
  not containing "Statistician" (indicator = 0). This suggests that on average the
  jobs that does not contain "Statistician" in the title may have a lower
  salary, but at the same time, there are large opportunities to reach a very high
  and less limited salary level. 
  

```{r,echo = FALSE, message=FALSE, warning=FALSE}
# Add the indicator to determine whether the title contains "Statistician"
Adzuna_data$Statistician <- ifelse(grepl("Statistician",Adzuna_data$title) == "TRUE", 1, 0)

ggplot(Adzuna_data) +
  geom_boxplot(aes(x = factor(Statistician), y = salary_max, fill=factor(Statistician))) +
  ylab("Maximum Salary") +
  xlab("Statistician Indicator") +
  ggtitle("Fig. 1: Maximum Salary Distributions for Statistician and Non-statistician") +
  scale_fill_discrete(name = "Statistician Indicator", labels = c("Job Title Not Containing Statistician", "Job Title Containing Statistician"))
```

### Required Skills & Level 


  On The Muse website, we could obtain the level of the positions for each job 
  posting, which includes internship, entry, mid, senior, management. From Table 1
  below, we can see the number of job posting in each level among the sampled pages,
  and it seems that under the data science category there are very few positions
  available for the entry level.
  

  
  
```{r, echo = FALSE}
level_num <- muse_data %>% 
  group_by(level) %>% 
  tally()
kableExtra::kable(level_num, caption = "Number of job posting in each level")
```
  
  Since programming skills are very crucial in data related positions, we could
  explore the occurrence frequency of specific skills in the job description among
  different position levels. More specifically, 4 commonly-used programing languages
  are tested, which are SQL, Python, SAS and R.
  
  
```{r, echo = FALSE, message = FALSE}
# Whether SQL is mentioned in the job description
muse_data$sql <- grepl("SQL",muse_data$contents)
# Whether Python is mentioned in the job description
muse_data$Python <- grepl("Python",muse_data$contents)
muse_data$SAS <- grepl("SAS",muse_data$contents)
muse_data$R <- grepl("\\bR\\b",muse_data$contents)
```



```{r, message=FALSE, echo = FALSE}
ggplot(muse_data, aes(level, fill = sql)) +
  geom_bar()
```

```{r, message=FALSE, echo = FALSE}
ggplot(muse_data, aes(level, fill = Python)) +
  geom_bar()
```

```{r, message=FALSE, echo = FALSE}
ggplot(muse_data, aes(level, fill = analysis)) +
  geom_bar()
```

```{r, echo = FALSE, message=FALSE, warning=FALSE}
text <- muse_data$contents
docs <- Corpus(VectorSource(text))

docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("the", "will", "are", "you", "this"))

dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)

set.seed(1997) 
wordcloud(words = df$word, freq = df$freq, min.freq = 1, max.words=180, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```

# Word Cloud
```{r, warning=FALSE, message=FALSE, echo = FALSE}
# Create a vector containing only the text
text_uk <- Adzuna_data$description
# Create a corpus  
docs_uk <- Corpus(VectorSource(text_uk))
# Clean the data -- remove some words, symbols & spaces
docs_uk <- docs_uk %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs_uk <- tm_map(docs_uk, removeWords, c("the", "will", "are", "you", "this"))
docs_uk <- tm_map(docs_uk, content_transformer(tolower))
docs_uk <- tm_map(docs_uk, removeWords, stopwords("english"))
```

```{r, echo = FALSE}
# Create a document-term-matrix
dtm_uk <- TermDocumentMatrix(docs_uk) 
matrix_uk <- as.matrix(dtm_uk) 
words_uk <- sort(rowSums(matrix_uk),decreasing=TRUE) 
df_uk <- data.frame(word = names(words_uk),freq=words_uk)
```

```{r, warning=FALSE, message=FALSE, echo = FALSE}
# For reproducibility 
set.seed(1997) 
wordcloud(words = df_uk$word, freq = df_uk$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```

### Adzuna Data -- US

```{r, echo = FALSE, message=FALSE, warning=FALSE}
text_us <- Adzuna_us$description
docs_us <- Corpus(VectorSource(text_us))

docs_us <- docs_us %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs_us <- tm_map(docs_us, removeWords, c("the", "will", "are", "you", "this"))

docs_us <- tm_map(docs_us, content_transformer(tolower))
docs_us <- tm_map(docs_us, removeWords, stopwords("english"))

dtm_us <- TermDocumentMatrix(docs_us) 
matrix_us <- as.matrix(dtm_us) 
words_us <- sort(rowSums(matrix_us),decreasing=TRUE) 
df_us <- data.frame(word = names(words_us),freq=words_us)

set.seed(1997) 
wordcloud(words = df_us$word, freq = df_us$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```

# Discussion 

# Limitations & Future Direction 

* Since for some job sites, there are daily limitations in utilization when using
API to query the data, it takes long time to get the data. Also, since the job 
sites have new updates every day, it is almost impossible to have the complete/full
data. Therefore, it could been developed into a more long-term project in the future,
which can have some updates on a regular base. 

*

# Shiny APP Description 

# Bibliography

1. Jeroen Ooms (2014). The jsonlite Package: A Practical and Consistent Mapping Between JSON Data and R
  Objects. arXiv:1403.2805 [stat.CO] URL https://arxiv.org/abs/1403.2805.
  
2. Hadley Wickham (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version
  1.4.0. https://CRAN.R-project.org/package=stringr
  
3. Wikipedia (2020). *Adzuna* [online]. Available from: https://en.wikipedia.org/wiki/Adzuna [accessed 13 December 2020].

4. Wikipedia (2020). *The Muse* [online]. Available from: https://en.wikipedia.org/wiki/The_Muse_(website) [accessed 13 December 2020].

5. Hao Zhu (2019). kableExtra: Construct Complex Table with 'kable' and Pipe Syntax. R package version 1.1.0.
  https://CRAN.R-project.org/package=kableExtra
  
6. Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686,
  https://doi.org/10.21105/joss.01686
  
7.  Ian Fellows (2018). wordcloud: Word Clouds. R package version 2.6.
  https://CRAN.R-project.org/package=wordcloud

8. Erich Neuwirth (2014). RColorBrewer: ColorBrewer Palettes. R package version 1.1-2.
  https://CRAN.R-project.org/package=RColorBrewer
  
9. Ingo Feinerer and Kurt Hornik (2020). tm: Text Mining Package. R package version 0.7-8.
  https://CRAN.R-project.org/package=tm
  
10. Towards Data Science (2019). *How to Generate Word Clouds in R* [online]. Available from: https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a [accessed 13 December 2020].

11. Stats And R (2020). *Draw a word cloud with a R Shiny app* [online]. Available from: https://www.statsandr.com/blog/draw-a-word-cloud-with-a-shiny-app/ [accessed 13 December 2020].



